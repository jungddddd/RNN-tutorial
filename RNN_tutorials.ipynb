{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 모델의 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN output shape: torch.Size([1, 200, 20])\n",
      "RNN hn shape: torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# RNN 정의\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "# 임의의 입력\n",
    "x = torch.randn(1, 200, 10)  # 배치 크기: 1, 시퀀스 길이: 200, 입력 차원: 10\n",
    "# t_1..t_200까지 순차적으로 10차원의 데이터가 들어가도록 만들겠다는 뜻.\n",
    "\n",
    "# 모델 실행\n",
    "output, hn = rnn(x)\n",
    "# output은 모든 time 에서 나오는 모든 h들을 모아놓은 것\n",
    "# hn은 200번에 해당하는 모든 데이터 포인트들을 다 지나간다음에 나오는 녀석\n",
    "\n",
    "print(f\"RNN output shape: {output.shape}\")\n",
    "print(f\"RNN hn shape: {hn.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2837,  0.5846, -0.7810, -0.1580, -0.5131,  0.5477,  0.3961, -0.2207,\n",
       "        -0.6812,  0.4416, -0.2011, -0.7379,  0.0648,  0.1456,  0.3722,  0.2928,\n",
       "        -0.5122,  0.2380, -0.6878,  0.2645], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2837,  0.5846, -0.7810, -0.1580, -0.5131,  0.5477,  0.3961, -0.2207,\n",
       "        -0.6812,  0.4416, -0.2011, -0.7379,  0.0648,  0.1456,  0.3722,  0.2928,\n",
       "        -0.5122,  0.2380, -0.6878,  0.2645], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM output shape: torch.Size([1, 200, 20])\n",
      "LSTM hn shape: torch.Size([1, 1, 20])\n",
      "LSTM cn shape: torch.Size([1, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "# LSTM 정의\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "# 임의의 입력\n",
    "x = torch.randn(1, 200, 10)  # 배치 크기: 1, 시퀀스 길이: 3, 입력 차원: 10\n",
    "# t_1..t_200까지 순차적으로 10차원의 데이터가 들어가도록 만들겠다는 뜻.\n",
    "\n",
    "# 모델 실행\n",
    "output, (hn, cn) = lstm(x)\n",
    "# 마찬가지로 output은 모든 time 에서 나오는 모든 h들을 모아놓은 것\n",
    "# LSTM의 경우는 hidden state 에 더해서 channel state 도 있음에 유의\n",
    "\n",
    "print(f\"LSTM output shape: {output.shape}\")\n",
    "print(f\"LSTM hn shape: {hn.shape}\")\n",
    "print(f\"LSTM cn shape: {cn.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0045, -0.0431, -0.0748, -0.0456, -0.1018,  0.1048, -0.0256,  0.1841,\n",
       "         0.0415,  0.2195,  0.0911, -0.2329,  0.0323,  0.1803,  0.0380,  0.0689,\n",
       "         0.1565, -0.0906,  0.2033, -0.1187], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0045, -0.0431, -0.0748, -0.0456, -0.1018,  0.1048, -0.0256,  0.1841,\n",
       "         0.0415,  0.2195,  0.0911, -0.2329,  0.0323,  0.1803,  0.0380,  0.0689,\n",
       "         0.1565, -0.0906,  0.2033, -0.1187], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0126, -0.0754, -0.1890, -0.1230, -0.2577,  0.1880, -0.0419,  0.3580,\n",
       "         0.1312,  0.5014,  0.2118, -0.5459,  0.0674,  0.4726,  0.0581,  0.1425,\n",
       "         0.2614, -0.1892,  0.5311, -0.2322], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반적인 모델의 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hid_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.LSTM = nn.LSTM(input_size = input_size, hidden_size = hid_size, num_layers=2,\n",
    "                            batch_first=True)   \n",
    "        self.fc1 = nn.Linear(self.hid_size, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x, (h,c) = self.LSTM(x)  # [bs, ml, hid_size]\n",
    "        x = F.relu(self.fc1(x[:,-1,:]))   # [bs, ml, hid_size]\n",
    "        # x = F.relu(self.fc1(h))   # [bs, ml, hid_size]\n",
    "        out = self.fc2(x)  \n",
    "        return out  # [bs, output_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN모델을 통한 주가 예측 (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (0.2.24)\n",
      "Requirement already satisfied: scikit-learn in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (4.9.3)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (2.0.3)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (2.3.8)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.26 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (2.28.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from yfinance) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from requests>=2.26->yfinance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from requests>=2.26->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from requests>=2.26->yfinance) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dongjaekim/miniforge3/lib/python3.10/site-packages (from requests>=2.26->yfinance) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T21:18:57.273311Z",
     "start_time": "2023-07-17T21:18:54.265725Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch  0 MSE:  0.013738235458731651\n",
      "Epoch  1 MSE:  0.012976775877177715\n",
      "Epoch  2 MSE:  0.012270951643586159\n",
      "Epoch  3 MSE:  0.011611342430114746\n",
      "Epoch  4 MSE:  0.010990006849169731\n",
      "Epoch  5 MSE:  0.010399633087217808\n",
      "Epoch  6 MSE:  0.00983443483710289\n",
      "Epoch  7 MSE:  0.00929019320756197\n",
      "Epoch  8 MSE:  0.00876376498490572\n",
      "Epoch  9 MSE:  0.008252782747149467\n",
      "Epoch  10 MSE:  0.007755504455417395\n",
      "Epoch  11 MSE:  0.007270782254636288\n",
      "Epoch  12 MSE:  0.006798103451728821\n",
      "Epoch  13 MSE:  0.006337698549032211\n",
      "Epoch  14 MSE:  0.005890676286071539\n",
      "Epoch  15 MSE:  0.00545924948528409\n",
      "Epoch  16 MSE:  0.005047030746936798\n",
      "Epoch  17 MSE:  0.004659315105527639\n",
      "Epoch  18 MSE:  0.00430311867967248\n",
      "Epoch  19 MSE:  0.0039863986894488335\n",
      "Epoch  20 MSE:  0.003715324215590954\n",
      "Epoch  21 MSE:  0.0034894058480858803\n",
      "Epoch  22 MSE:  0.003297035349532962\n",
      "Epoch  23 MSE:  0.003117527114227414\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = yf.download('AAPL', '2000-01-01', '2023-07-01')\n",
    "data = data['Close'].values\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# 설정값 정의\n",
    "seq_length = 2000\n",
    "pred_length = 200\n",
    "hidden_dim = 50\n",
    "n_features = 1\n",
    "\n",
    "# 학습 데이터 생성\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-pred_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[(i+seq_length):(i+seq_length+pred_length)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "x, y = create_sequences(data, seq_length, pred_length)\n",
    "x = torch.tensor(x).float().view(-1, seq_length, n_features)\n",
    "y = torch.tensor(y).float()\n",
    "\n",
    "train_x = x[:len(x)//2]\n",
    "test_x = x[len(x)//2:]\n",
    "train_y = y[:len(y)//2]\n",
    "test_y = y[len(y)//2:]\n",
    "\n",
    "# 모델 정의\n",
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, seq_length, pred_length):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_length\n",
    "        self.pred_len = pred_length\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_dim)\n",
    "        self.linear = nn.Linear(in_features=hidden_dim, out_features=pred_length)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        lstm_out, _ = self.lstm(sequences.view(len(sequences), self.seq_len, -1))\n",
    "        last_time_step = lstm_out.view(self.seq_len, len(sequences), self.hidden_dim)[-1]\n",
    "        y_pred = self.linear(last_time_step)\n",
    "        return y_pred\n",
    "\n",
    "model = StockPredictor(n_features, hidden_dim, seq_length, pred_length)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 학습\n",
    "model.train()\n",
    "for epoch in range(50):  # 50번의 epoch 동안 학습\n",
    "    model.zero_grad()\n",
    "    y_pred = model(train_x)\n",
    "    loss = criterion(y_pred.float(), train_y)\n",
    "    print(\"Epoch \", epoch, \"MSE: \", loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 평가\n",
    "model.eval()\n",
    "test_preds = model(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "range_future = len(test_y)\n",
    "\n",
    "plt.plot(np.arange(200), test_y[0,:].numpy(), label='True Future')\n",
    "plt.plot(np.arange(200), test_preds[0,:].detach().numpy(), label='Predicted Future')\n",
    "\n",
    "plt.title('True Future vs Predicted Future')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Stock Price')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN을 이용한 문서의 분류 (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import * \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.datasets import imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_WORDS)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, max_words, emb_size, hid_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.max_words = max_words\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
    "        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n",
    "                            batch_first=True)   \n",
    "        self.fc1 = nn.Linear(self.hid_size, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x, (h,c) = self.LSTM(x)  # [bs, ml, hid_size]\n",
    "        x = F.relu(self.fc1(x[:,-1,:]))   # [bs, ml, hid_size]\n",
    "        out = self.fc2(x)  \n",
    "        return out  # [bs, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = criterion(y_, y)  # loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader): \n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  \n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "        test_loss += criterion(y_, y)\n",
    "        pred = y_.max(-1, keepdim=True)[1] \n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, acc, len(test_loader.dataset),\n",
    "        100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000  # imdb’s vocab_size \n",
    "MAX_LEN = 200      # max length\n",
    "BATCH_SIZE = 256\n",
    "EMB_SIZE = 128   # embedding size\n",
    "HID_SIZE = 128   # lstm hidden size\n",
    "\n",
    "\n",
    "model = Model(MAX_WORDS, EMB_SIZE, HID_SIZE).to(DEVICE)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_acc = 0.0 \n",
    "PATH = 'imdb model/model.pth'  #\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
